{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8493633e",
   "metadata": {},
   "source": [
    "# Policy Learning Only Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b566355",
   "metadata": {},
   "source": [
    "In this workflow, CFRL takes in an offline trajectory and then preprocesses the offline trajectory using `SequentialPreprocessor`. After that, the preprocessed trajectory is passed into `FQI` to train a counterfactually fair policy, which is the final output of the workflow. This workflow is appropriate if the user wants to train a policy using CFRL. The trained policy can be further evaluated on its value and counterfactual fairness, which is discussed in detail in the \"Assessing Policies Using Real Data\" notebook.\n",
    "\n",
    "We begin by importing the libraries needed for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6f8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this temporarily to import CFRL before it is officially published to PyPI\n",
    "import sys\n",
    "sys.path.append(\"E:/learning/university/MiSIL/CFRL Python Package/CFRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df934c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x163ad1ce2f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cfrl.reader import read_trajectory_from_dataframe, convert_trajectory_to_dataframe\n",
    "from cfrl.preprocessor import SequentialPreprocessor\n",
    "from cfrl.agents import FQI\n",
    "np.random.seed(10) # ensure reproducibility\n",
    "torch.manual_seed(10) # ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff76bcd",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6c556",
   "metadata": {},
   "source": [
    "In this demonstration, we use an offline trajectory generated from a `SyntheticEnvironment` using some pre-specified transition rules. Although it is actually synthesized, we treat it as if it is from some unknown environment for pedagogical convenience in this demonstration.\n",
    "\n",
    "The trajectory contains 500 individuals (i.e. $N=500$) and 10 transitions (i.e. $T=10$). The actions are binary ($0$ or $1$) and were sampled using a random policy that selects $0$ or $1$ randomly with equal probability. It is stored in a tabular format in a `.csv` file. The sensitive attribute variable is univariate, stored in the column `z1`. The legit values of the sensitive attribute are $0$ and $1$. The state variable is also univariate, stored in the column `state1`. The actions are stored in the column `action` and rewards in the column `reward`. The tabular data also includes an extra irrelevant column `timestamp`. \n",
    "\n",
    "We can load and view the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fca2afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>z1</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>state1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.324345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.524345</td>\n",
       "      <td>-0.813722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.613722</td>\n",
       "      <td>-0.526683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.326683</td>\n",
       "      <td>-0.464447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.264447</td>\n",
       "      <td>-2.075518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>5495</td>\n",
       "      <td>500.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.468460</td>\n",
       "      <td>-0.941954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>5496</td>\n",
       "      <td>500.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.430345</td>\n",
       "      <td>-2.536595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>5497</td>\n",
       "      <td>500.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.068298</td>\n",
       "      <td>-0.946557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>5498</td>\n",
       "      <td>500.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.273278</td>\n",
       "      <td>-0.709017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>5499</td>\n",
       "      <td>500.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.154508</td>\n",
       "      <td>-0.761890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     ID  timestamp   z1  action    reward    state1\n",
       "0              0    1.0        1.0  0.0     NaN       NaN  1.324345\n",
       "1              1    1.0        2.0  0.0     1.0  1.524345 -0.813722\n",
       "2              2    1.0        3.0  0.0     1.0 -0.613722 -0.526683\n",
       "3              3    1.0        4.0  0.0     1.0 -0.326683 -0.464447\n",
       "4              4    1.0        5.0  0.0     1.0 -0.264447 -2.075518\n",
       "...          ...    ...        ...  ...     ...       ...       ...\n",
       "5495        5495  500.0        7.0  1.0     1.0 -2.468460 -0.941954\n",
       "5496        5496  500.0        8.0  1.0     1.0 -1.430345 -2.536595\n",
       "5497        5497  500.0        9.0  1.0     0.0 -1.068298 -0.946557\n",
       "5498        5498  500.0       10.0  1.0     0.0 -0.273278 -0.709017\n",
       "5499        5499  500.0       11.0  1.0     0.0 -0.154508 -0.761890\n",
       "\n",
       "[5500 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = pd.read_csv('../data/sample_data_large_uni.csv')\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc25b3",
   "metadata": {},
   "source": [
    "We now read the trajectory from the tabular format into Trajectory Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67fe8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs, states, actions, rewards, ids = read_trajectory_from_dataframe(\n",
    "                                                data=trajectory, \n",
    "                                                z_labels=['z1'], \n",
    "                                                state_labels=['state1'], \n",
    "                                                action_label='action', \n",
    "                                                reward_label='reward', \n",
    "                                                id_label='ID', \n",
    "                                                T=10\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0226a",
   "metadata": {},
   "source": [
    "## Preprocessor Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066511b4",
   "metadata": {},
   "source": [
    "Before preprocessing the trajectory, we need to first train a preprocessor. To mitigate overfitting, we use a random subset of 250 individuals in the trajectory to train the preprocessor. The remaining 250 individuals will be actually preprocessed. We now form these two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0205842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    zs_train, zs_prepro, \n",
    "    states_train, states_prepro, \n",
    "    actions_train, actions_prepro, \n",
    "    rewards_train, rewards_prepro, \n",
    "    ids_train, ids_prepro\n",
    ") = train_test_split(zs, states, actions, rewards, ids, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75983d7",
   "metadata": {},
   "source": [
    "We now use the training set to train a `SequentialPreprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db1b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 41.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[-0.32461696,  0.57096795],\n",
       "         [ 0.22408737,  1.31944726],\n",
       "         [-0.75577441,  1.2936311 ],\n",
       "         ...,\n",
       "         [-0.65848739,  0.02353077],\n",
       "         [-0.64196535,  0.73095815],\n",
       "         [-0.40729995,  1.4571281 ]],\n",
       " \n",
       "        [[-0.2082495 ,  0.6873354 ],\n",
       "         [ 1.25568126,  2.39690117],\n",
       "         [-0.45044321,  1.84835638],\n",
       "         ...,\n",
       "         [-1.11037296,  0.43440456],\n",
       "         [-1.58189396,  0.45079704],\n",
       "         [-2.8786627 , -1.66740244]],\n",
       " \n",
       "        [[ 1.20425904,  2.09984394],\n",
       "         [-0.4995148 ,  1.66238533],\n",
       "         [ 1.24682442,  3.91839007],\n",
       "         ...,\n",
       "         [ 0.41160054,  2.41311652],\n",
       "         [-1.37682367,  0.37004053],\n",
       "         [ 0.36016143,  2.51716889]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.26530747,  2.16089238],\n",
       "         [-1.92501584,  0.22994996],\n",
       "         [-1.00322249,  1.14478895],\n",
       "         ...,\n",
       "         [-2.70255556, -1.07172801],\n",
       "         [-0.83831198,  0.45569243],\n",
       "         [-0.87000105,  0.26598686]],\n",
       " \n",
       "        [[ 0.93616403,  1.83174894],\n",
       "         [-0.56039299,  1.02950477],\n",
       "         [-0.93756035,  0.52739995],\n",
       "         ...,\n",
       "         [ 0.16796295,  2.12201015],\n",
       "         [-1.54496402,  0.11792355],\n",
       "         [-0.29965428,  0.71587086]],\n",
       " \n",
       "        [[ 0.90205486,  1.79763977],\n",
       "         [-0.27593696,  1.98100394],\n",
       "         [-0.28894577,  2.34886482],\n",
       "         ...,\n",
       "         [ 0.77412728,  3.29498461],\n",
       "         [-2.27353748,  1.35247842],\n",
       "         [-1.03102516,  1.13688195]]]),\n",
       " array([[ 0.03384968,  0.84198327,  0.14144528, ..., -1.62861994,\n",
       "         -0.36630844,  0.05746048],\n",
       "        [ 0.0570628 ,  1.9911891 ,  0.76756303, ...,  0.19270372,\n",
       "         -0.37809996, -0.21529483],\n",
       "        [ 1.79174967,  0.63732385,  1.10074373, ...,  0.28420918,\n",
       "          0.56308291, -0.5476369 ],\n",
       "        ...,\n",
       "        [ 1.86256401, -0.92760278,  0.0582493 , ...,  0.12394196,\n",
       "         -0.74293841, -0.08429932],\n",
       "        [ 0.55399481,  0.10603715, -0.21226242, ...,  0.53568457,\n",
       "          0.48144231, -0.25481419],\n",
       "        [ 1.47607445,  0.98414954,  0.4825289 , ...,  0.80053472,\n",
       "          2.29818616, -0.07862115]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = SequentialPreprocessor(z_space=[[0], [1]], \n",
    "                            num_actions=2, \n",
    "                            cross_folds=1, \n",
    "                            mode='single', \n",
    "                            reg_model='nn')\n",
    "sp.train_preprocessor(zs=zs_train, xs=states_train, actions=actions_train, rewards=rewards_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493c2d4",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d8744",
   "metadata": {},
   "source": [
    "We now train a counterfactually fair policy using the preprocessor `sp` and the set of trajectory data that is not used to train `sp`. We begin by initializing an `FQI` agent with `sp` as its internal preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1443cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FQI(num_actions=2, model_type='nn', preprocessor=sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c88801",
   "metadata": {},
   "source": [
    "We now perform training. Since we set `preprocess=True` in `train()`, `agent` will use its internal preprocessor (i.e. `sp`) to automatically preprocess the input training trajectory before using the trajectory for policy learning. Therefore, we can directly pass in the unpreprocessed `states_prepro` and `rewards_prepro`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a43649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:48<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train(zs=zs_prepro, \n",
    "            xs=states_prepro, \n",
    "            actions=actions_prepro, \n",
    "            rewards=rewards_prepro, \n",
    "            max_iter=100, \n",
    "            preprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67cec9",
   "metadata": {},
   "source": [
    "## Decision-making Using the Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13343eb1",
   "metadata": {},
   "source": [
    "Now we have trained a counterfactually fair policy. We can use its `act()` method to make decisions. At actual deployment, the single-step trajectory passed into `act()` is often generated from the real enviornment at the time. However, in this part, we reuse the training trajectory data for convenience.\n",
    "\n",
    "We first take actions for the initial time step (i.e. $a_0$). Again, we only pass in unpreprocessed states and rewards because they will be automatically preprocessed inside `act()` before being used for decision-making. We set `xtm1` and `atm1` to `None`. This tells `agent` that we are taking the initial action and that the buffer of the internal preprocessor should be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f21bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = agent.act(z=zs, \n",
    "               xt=states[:, 0], \n",
    "               xtm1=None, \n",
    "               atm1=None, \n",
    "               preprocess=True)\n",
    "a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb0685",
   "metadata": {},
   "source": [
    "We now take the second action. We still pass in unpreprocessed states and rewards, and we specify non-`None` `xtm1` and `atm1` to tell `agent` not to reset the buffer of its internal preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80ff7a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = agent.act(z=zs, \n",
    "               xt=states[:, 1], \n",
    "               xtm1=states[:, 0], \n",
    "               atm1=a0, \n",
    "               preprocess=True)\n",
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063315a",
   "metadata": {},
   "source": [
    "## Alternative: Using All Individuals for Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a30b10",
   "metadata": {},
   "source": [
    "Sometimes, the number of individuals in the trajectory is small. In this case, if we only use a subset of individuals for policy learning, the amount of training trajectory data might be too small to be useful for policy learning. Fortunately, when we set `cross_folds` to a relatively large number, we can directly preprocess all individuals using the `train_preprocessor()` function, and the preprocessed trajectory can be used for policy learning.\n",
    "\n",
    "When `cross_folds=K` where `K` is greater than 1, `train_preprocessor()` will internally divide the training data into `K` folds. For each $i=1,\\dots,K$, it trains a transition dynamics model based on all the folds other than the $i$-th one, and this model is then used to preprocess data in the $i$-th fold. This results in `K` folds of preprocessed data, each of which is processed using a model that is trained on the other folds. These `K` folds of preprocessed data are then combined and returned by `train_preprocessor()`. This method allows us to preprocess all individuals in the trajectory while reducing overfitting.\n",
    "\n",
    "To use this functionality, we first initialize a `SequentialPreprocessor` with `cross_folds` greater than 1. We use `cross_folds=5` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cab5cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cf5 = SequentialPreprocessor(z_space=[[0], [1]], \n",
    "                                num_actions=2, \n",
    "                                cross_folds=5, \n",
    "                                mode='single', \n",
    "                                reg_model='nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e84b0",
   "metadata": {},
   "source": [
    "We now simultaneously train the preprocessor and preprocess all individuals in the trajectory using the precedure described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "390ab5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.67it/s]\n",
      "100%|██████████| 1000/1000 [00:36<00:00, 27.70it/s]\n",
      "100%|██████████| 1000/1000 [00:38<00:00, 26.02it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.85it/s]\n",
      "100%|██████████| 1000/1000 [00:37<00:00, 26.71it/s]\n"
     ]
    }
   ],
   "source": [
    "states_tilde_cf5, rewards_tilde_cf5 = sp_cf5.train_preprocessor(zs=zs, \n",
    "                                                                xs=states, \n",
    "                                                                actions=actions, \n",
    "                                                                rewards=rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f7cae",
   "metadata": {},
   "source": [
    "We now train a policy using `FQI` and the preprocessed data. Note that the input training data `state_tilde` and `rewards_tilde` are already preprocessed. Thus, we set `preprocess=False` during training so that the input trajectory will not be preprocessed again by the internal preprocessor (i.e. `sp_cf5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2be5f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "agent_cf5 = FQI(num_actions=2, model_type='nn', preprocessor=sp_cf5)\n",
    "agent_cf5.train(zs=zs, \n",
    "                xs=states_tilde_cf5, \n",
    "                actions=actions, \n",
    "                rewards=rewards_tilde_cf5, \n",
    "                max_iter=100, \n",
    "                preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f01bf3",
   "metadata": {},
   "source": [
    "We now take the initial action using `agent_cf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a2f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = agent_cf5.act(z=zs, \n",
    "                   xt=states[:, 0], \n",
    "                   xtm1=None, \n",
    "                   atm1=None, \n",
    "                   preprocess=True)\n",
    "a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbef31",
   "metadata": {},
   "source": [
    "We now take the second action using `agent_cf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97cb571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = agent_cf5.act(z=zs, \n",
    "                   xt=states[:, 1], \n",
    "                   xtm1=states[:, 0], \n",
    "                   atm1=a0, \n",
    "                   preprocess=True)\n",
    "a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
