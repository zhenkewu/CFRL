{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8493633e",
   "metadata": {},
   "source": [
    "# Policy Learning Only Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b566355",
   "metadata": {},
   "source": [
    "In this workflow, CFRL takes in an offline trajectory and then preprocesses the offline trajectory using `SequentialPreprocessor`. After that, the preprocessed trajectory is passed into `FQI` to train a counterfactually fair policy, which is the final output of the workflow. This workflow is appropriate if the user wants to train a policy using CFRL. The trained policy can be further evaluated on its value and counterfactual fairness, which is discussed in detail in the \"Assessing Policies Using Real Data\" workflow later in this section.\n",
    "\n",
    "We begin by importing the liberaries needed for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6f8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this temporarily to import CFRL before it is officially published to PyPI\n",
    "import sys\n",
    "sys.path.append(\"E:/learning/university/MiSIL/CFRL Python Package/CFRL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df934c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pytorch as torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from CFRL.reader import read_trajectory_from_dataframe, convert_trajectory_to_dataframe\n",
    "from CFRL.preprocessor import SequentialPreprocessor\n",
    "from CFRL.agents import FQI\n",
    "np.random.seed(1) # ensure reproducibility\n",
    "#torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff76bcd",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6c556",
   "metadata": {},
   "source": [
    "In this demonstration, we use an offline trajectory generated from a `SyntheticEnvironment` using some pre-specified transition rules. Although it is actually synthesized, we treat it as if it is from some unknown environment for pedagogical convenience in this demonstration.\n",
    "\n",
    "The trajectory contains 500 individuals (i.e. $N=500$) and 10 transitions (i.e. $T=10$). The actions are binary (0 or 1) and were sampled using a random policy that selects 0 or 1 randomly with equal probability. It is stored in a tabular format in a `.csv` file. The sensitive attribute variable is bivariate, stored in columns `z1` and `z2`. The legit values of the sensitive attribute are $[0, 0]$, $[1, 0]$, $[0, 1]$, and $[1, 1]$. The state variable is tri-variate, stored in columns `state1`, `state2`, and `state3`. The actions are stored in the column `action` and rewards in the column `reward`. The tabular data also includes an extra inrrelevant column `timestamp`. \n",
    "\n",
    "We can load and view the tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fca2afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>state1</th>\n",
       "      <th>state2</th>\n",
       "      <th>state3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.124345</td>\n",
       "      <td>-0.111756</td>\n",
       "      <td>-0.028172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.380339</td>\n",
       "      <td>-0.071876</td>\n",
       "      <td>0.545250</td>\n",
       "      <td>-0.020279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.849111</td>\n",
       "      <td>-1.084077</td>\n",
       "      <td>-1.696634</td>\n",
       "      <td>-1.179136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.421291</td>\n",
       "      <td>-2.317520</td>\n",
       "      <td>-1.787875</td>\n",
       "      <td>-2.148363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.142691</td>\n",
       "      <td>-2.936506</td>\n",
       "      <td>-3.603797</td>\n",
       "      <td>-3.590126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>5495</td>\n",
       "      <td>500.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.563265</td>\n",
       "      <td>-4.024293</td>\n",
       "      <td>-6.587401</td>\n",
       "      <td>-3.859436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5496</th>\n",
       "      <td>5496</td>\n",
       "      <td>500.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.073520</td>\n",
       "      <td>-5.952644</td>\n",
       "      <td>-5.854450</td>\n",
       "      <td>-4.218220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5497</th>\n",
       "      <td>5497</td>\n",
       "      <td>500.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.691358</td>\n",
       "      <td>-5.687570</td>\n",
       "      <td>-6.008377</td>\n",
       "      <td>-5.618730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5498</th>\n",
       "      <td>5498</td>\n",
       "      <td>500.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.394408</td>\n",
       "      <td>-7.551435</td>\n",
       "      <td>-6.816310</td>\n",
       "      <td>-6.740886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5499</th>\n",
       "      <td>5499</td>\n",
       "      <td>500.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.989511</td>\n",
       "      <td>-6.202696</td>\n",
       "      <td>-8.487149</td>\n",
       "      <td>-7.020361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     ID  timestamp   z1   z2  action     reward    state1  \\\n",
       "0              0    1.0        1.0  0.0  0.0     NaN        NaN  2.124345   \n",
       "1              1    1.0        2.0  0.0  0.0     1.0   3.380339 -0.071876   \n",
       "2              2    1.0        3.0  0.0  0.0     0.0   1.849111 -1.084077   \n",
       "3              3    1.0        4.0  0.0  0.0     0.0  -4.421291 -2.317520   \n",
       "4              4    1.0        5.0  0.0  0.0     1.0  -5.142691 -2.936506   \n",
       "...          ...    ...        ...  ...  ...     ...        ...       ...   \n",
       "5495        5495  500.0        7.0  0.0  0.0     0.0 -12.563265 -4.024293   \n",
       "5496        5496  500.0        8.0  0.0  0.0     0.0 -14.073520 -5.952644   \n",
       "5497        5497  500.0        9.0  0.0  0.0     0.0 -16.691358 -5.687570   \n",
       "5498        5498  500.0       10.0  0.0  0.0     0.0 -18.394408 -7.551435   \n",
       "5499        5499  500.0       11.0  0.0  0.0     1.0 -20.989511 -6.202696   \n",
       "\n",
       "        state2    state3  \n",
       "0    -0.111756 -0.028172  \n",
       "1     0.545250 -0.020279  \n",
       "2    -1.696634 -1.179136  \n",
       "3    -1.787875 -2.148363  \n",
       "4    -3.603797 -3.590126  \n",
       "...        ...       ...  \n",
       "5495 -6.587401 -3.859436  \n",
       "5496 -5.854450 -4.218220  \n",
       "5497 -6.008377 -5.618730  \n",
       "5498 -6.816310 -6.740886  \n",
       "5499 -8.487149 -7.020361  \n",
       "\n",
       "[5500 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory = pd.read_csv('../data/sample_data_large_multi.csv')\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc25b3",
   "metadata": {},
   "source": [
    "We now read the trajectory from the tabular format into Trajectory Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fe8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs, states, actions, rewards, ids = read_trajectory_from_dataframe(\n",
    "                                                data=trajectory, \n",
    "                                                z_labels=['z1', 'z2'], \n",
    "                                                state_labels=['state1', 'state2', 'state3'], \n",
    "                                                action_label='action', \n",
    "                                                reward_label='reward', \n",
    "                                                id_label='ID', \n",
    "                                                T=10\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0226a",
   "metadata": {},
   "source": [
    "## Preprocessor Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066511b4",
   "metadata": {},
   "source": [
    "Before preprocessing the trajectory, we need to first train a preprocessor. To mitigate overfitting, we use a random subset of 250 individuals in the trajectory to train the preprocessor. The remaining 250 individuals will be actually preprocessed. We now form these two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0205842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    zs_train, zs_prepro, \n",
    "    states_train, states_prepro, \n",
    "    actions_train, actions_prepro, \n",
    "    rewards_train, rewards_prepro, \n",
    "    ids_train, ids_prepro\n",
    ") = train_test_split(zs, states, actions, rewards, ids, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75983d7",
   "metadata": {},
   "source": [
    "We now use the training set to train a `SequentialPreprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db1b22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  0.29730609,   0.64565182,  -1.68102797, ...,   2.47447229,\n",
       "            2.52162402,   0.375634  ],\n",
       "         [ -1.35034815,  -2.25309663,  -1.63382889, ...,   2.77911791,\n",
       "            1.54900343,   1.38914679],\n",
       "         [ -3.42806811,  -4.52740321,  -3.80843044, ...,   1.76331143,\n",
       "            0.3825182 ,   1.79272496],\n",
       "         ...,\n",
       "         [ -6.80947176,  -7.44296927,  -6.91436753, ...,   5.74308356,\n",
       "            3.70575099,   4.58800617],\n",
       "         [ -8.76876788,  -7.18474406,  -8.88386579, ...,   3.65733714,\n",
       "            4.32011253,   4.03171258],\n",
       "         [ -9.04944974,  -9.08450819,  -9.40940367, ...,   4.41984767,\n",
       "            4.20582365,   4.18209709]],\n",
       " \n",
       "        [[  0.30006061,   0.50738898,   0.77566408, ...,   2.47722681,\n",
       "            2.38336118,   2.83232604],\n",
       "         [ -1.27571934,  -1.32202527,  -2.06041935, ...,   3.00406546,\n",
       "            2.32318667,   1.76551351],\n",
       "         [ -3.04574095,  -4.47713707,  -2.44819125, ...,   2.43019856,\n",
       "            1.01633669,   3.36437742],\n",
       "         ...,\n",
       "         [ -9.70628283,  -8.52692268, -10.77813067, ...,   3.35788538,\n",
       "            3.79405953,   2.57327781],\n",
       "         [ -9.88815959,  -9.48028015,  -9.37377393, ...,   4.35848538,\n",
       "            4.17125131,   4.85264838],\n",
       "         [ -9.13445216, -10.46861851,  -9.79687712, ...,   6.36153573,\n",
       "            3.98819483,   5.70147923]],\n",
       " \n",
       "        [[  2.35577998,   0.8234965 ,  -1.11028779, ...,   4.53294618,\n",
       "            2.69946869,   0.94637418],\n",
       "         [ -3.53511536,   1.20576471,  -2.40420579, ...,   1.1444    ,\n",
       "            3.3355934 ,   0.99101389],\n",
       "         [ -4.10865966,  -2.94901027,  -4.36857571, ...,   3.49983772,\n",
       "            2.27395235,   2.10805784],\n",
       "         ...,\n",
       "         [-10.32877841,  -9.79055982,  -8.380596  , ...,   3.94202886,\n",
       "            4.02599043,   5.90716665],\n",
       "         [-11.00081264, -10.48607304, -11.46929903, ...,   4.03658713,\n",
       "            3.85289142,   3.8311656 ],\n",
       "         [-11.29419917,  -8.7315431 , -12.54921582, ...,   4.11021739,\n",
       "            5.78892394,   3.25673959]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  1.08687877,  -0.81371933,  -0.37529462, ...,   3.26404497,\n",
       "            1.06225287,   1.68136735],\n",
       "         [ -1.83798772,  -1.92986238,  -1.00193343, ...,   1.61241526,\n",
       "            1.66382008,   3.15423634],\n",
       "         [ -3.61096736,  -4.07601403,  -2.95788012, ...,   2.19556479,\n",
       "            2.05749847,   2.47192897],\n",
       "         ...,\n",
       "         [ -8.21395454,  -9.44197225, -11.0639867 , ...,   6.41406479,\n",
       "            4.34650422,   3.69709309],\n",
       "         [-12.50219568, -10.36501936,  -8.39720254, ...,   3.24530092,\n",
       "            4.45654818,   7.36314483],\n",
       "         [ -9.66616686, -11.70330744, -11.89971172, ...,   4.98621265,\n",
       "            2.99539776,   4.47596968]],\n",
       " \n",
       "        [[  0.52599999,  -0.84869645,   1.80253554, ...,   2.70316619,\n",
       "            1.02727575,   3.8591975 ],\n",
       "         [  0.54269952,   0.80348042,   0.4486412 , ...,   3.7873649 ,\n",
       "            4.20808545,   3.94885447],\n",
       "         [ -2.11077897,  -2.37800658,  -1.60129975, ...,   3.42471071,\n",
       "            2.11152792,   3.77547325],\n",
       "         ...,\n",
       "         [ -5.74581748,  -5.60359822,  -5.5942163 , ...,   6.02955836,\n",
       "            5.40940466,   5.79172854],\n",
       "         [ -6.86498505,  -7.39501148,  -5.57834589, ...,   5.62459272,\n",
       "            3.8205809 ,   6.69566143],\n",
       "         [ -8.29057402,  -6.90579674,  -8.5883513 , ...,   4.28785091,\n",
       "            4.74601105,   4.17883439]],\n",
       " \n",
       "        [[  1.03869025,   1.01216151,  -0.45934013, ...,   3.21585645,\n",
       "            2.88813371,   1.59732184],\n",
       "         [ -0.59712587,  -1.36945037,  -1.26988847, ...,   3.83630933,\n",
       "            2.57623144,   3.22658449],\n",
       "         [ -4.07742016,  -1.64499448,  -1.27288719, ...,   1.51150806,\n",
       "            3.76476743,   4.95414786],\n",
       "         ...,\n",
       "         [ -8.34597891,  -8.39821287,  -9.99851179, ...,   4.85570246,\n",
       "            4.50203566,   3.20129585],\n",
       "         [ -9.10984322,  -8.27224625, -10.53246567, ...,   4.54478076,\n",
       "            5.11119424,   3.08325317],\n",
       "         [-10.90277929, -10.44304996,  -9.7895374 , ...,   4.1777609 ,\n",
       "            3.8157844 ,   4.85593369]]]),\n",
       " array([[ 4.38515979,  2.00711159, -2.38659761, ..., -4.33539711,\n",
       "         -2.92243191, -4.63204309],\n",
       "        [ 3.04366748,  3.13167016,  0.38787276, ..., -4.81709774,\n",
       "         -5.03930197, -5.39092969],\n",
       "        [ 7.20675874,  2.66232541,  2.07711314, ..., -4.16616677,\n",
       "         -4.9476514 , -9.68175991],\n",
       "        ...,\n",
       "        [ 4.0640962 ,  1.54838703, -0.07029854, ..., -4.7897793 ,\n",
       "         -4.79122614, -5.00147528],\n",
       "        [ 5.3239073 ,  8.2080976 ,  1.95929539, ...,  6.88038546,\n",
       "          4.21743617, -0.22001565],\n",
       "        [ 7.2577816 ,  4.032121  ,  2.11520417, ..., -5.38377925,\n",
       "         -8.07592072, -6.26942307]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = SequentialPreprocessor(z_space=[[0, 0], [0, 1], [1, 0], [1, 1]], \n",
    "                            num_actions=2, \n",
    "                            cross_folds=1, \n",
    "                            mode='single', \n",
    "                            reg_model='nn')\n",
    "sp.train_preprocessor(zs=zs_train, xs=states_train, actions=actions_train, rewards=rewards_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493c2d4",
   "metadata": {},
   "source": [
    "## Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d8744",
   "metadata": {},
   "source": [
    "We now train a counterfactually fair policy using the preprocessor `sp` and the set of trajectory data that is not used to train `sp`. We begin by initializing an `FQI` agent with `sp` as its internal preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1443cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FQI(num_actions=2, model_type='nn', preprocessor=sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c88801",
   "metadata": {},
   "source": [
    "We now perform training. Since we set `preprocess=True` in `train()`, `agent` will use its internal preprocessor (i.e. `sp`) to automatically preprocess the input training trajectory before using the trajectory for policy learning. Therefore, we can directly pass in the unpreprocessed `states_prepro` and `rewards_prepro`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a43649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "agent.train(zs=zs_prepro, \n",
    "            xs=states_prepro, \n",
    "            actions=actions_prepro, \n",
    "            rewards=rewards_prepro, \n",
    "            max_iter=100, \n",
    "            preprocess=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67cec9",
   "metadata": {},
   "source": [
    "## Decision-making Using the Trained Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13343eb1",
   "metadata": {},
   "source": [
    "Now we have trained a counterfactually fair policy. We can use its `act()` method to make decisions. At actual deployment, the single-step trajectory passed into `act()` is often real environment data generated at the time. However, in this part, we reuse the training trajectory data for demonstration purposes.\n",
    "\n",
    "We first take actions for the initial time step (i.e. $a_0$). Again, we only pass in unpreprocessed states and rewards because they will be automatically preprocessed inside `act()` before being used for decision-making. We set `xtm1` and `atm1` to `None`. This tells `agent` that we are taking the initial action and that the buffer of the internal preprocessor should be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f21bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = agent.act(z=zs, \n",
    "               xt=states[:, 0], \n",
    "               xtm1=None, \n",
    "               atm1=None, \n",
    "               preprocess=True)\n",
    "a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb0685",
   "metadata": {},
   "source": [
    "We now take the second action. We still pass in unpreprocessed states and rewards, and we specify non-`None` `xtm1` and `atm1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ff7a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = agent.act(z=zs, \n",
    "               xt=states[:, 1], \n",
    "               xtm1=states[:, 0], \n",
    "               atm1=a0, \n",
    "               preprocess=True)\n",
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063315a",
   "metadata": {},
   "source": [
    "## Alternative: Using All Individuals for Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a30b10",
   "metadata": {},
   "source": [
    "Sometimes, the number of individuals in the trajectory is small. In this case, if we only use a subset of individuals for policy learning, the amount of training trajectory data might be too small to be useful. Fortunately, when we have a relatively large number of `cross_folds`, we can directly preprocess all individuals using the `train_preprocessor()` function, and the preprocessed trajectory can be used for policy learning.\n",
    "\n",
    "When `cross_folds=K` where `K` is greater than 1, `train_preprocessor()` will internally divide the training data into `K` folds. For each $i=1,\\dots,k$, it trains a model based on all the folds other than the $i$-th one, which is then used to preprocess data in the $i$-th fold. This results in `K` folds of preprocessed data, each of which is processed using a model that is trained on the other folds. These `K` folds of data are then combined and returned by `train_preprocessor()`. This method allows us to preprocess all individuals in the trajectory while reducing overfitting.\n",
    "\n",
    "To use this functionality, we first initialize a `SequentialPreprocessor` with `cross_folds` greater than 1. We use `cross_folds=5` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab5cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cf5 = SequentialPreprocessor(z_space=[[0, 0], [0, 1], [1, 0], [1, 1]], \n",
    "                                num_actions=2, \n",
    "                                cross_folds=5, \n",
    "                                mode='single', \n",
    "                                reg_model='nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e84b0",
   "metadata": {},
   "source": [
    "We now simultaneously train the preprocessor and preprocess all individuals in the trajectory using the precedure described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "390ab5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 22.76it/s]\n",
      "100%|██████████| 1000/1000 [00:44<00:00, 22.43it/s]\n",
      "100%|██████████| 1000/1000 [01:01<00:00, 16.25it/s]\n",
      "100%|██████████| 1000/1000 [00:47<00:00, 20.91it/s]\n",
      "100%|██████████| 1000/1000 [00:47<00:00, 20.92it/s]\n"
     ]
    }
   ],
   "source": [
    "states_tilde_cf5, rewards_tilde_cf5 = sp_cf5.train_preprocessor(zs=zs, \n",
    "                                                                xs=states, \n",
    "                                                                actions=actions, \n",
    "                                                                rewards=rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f7cae",
   "metadata": {},
   "source": [
    "We now train a policy using `FQI` and the preprocessed data. Note that we set `preprocess=False` during training because the training data `state_tilde` and `rewards_tilde` are already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be5f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:46<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "agent_cf5 = FQI(num_actions=2, model_type='nn', preprocessor=sp)\n",
    "agent_cf5.train(zs=zs, \n",
    "                xs=states_tilde_cf5, \n",
    "                actions=actions, \n",
    "                rewards=rewards_tilde_cf5, \n",
    "                max_iter=100, \n",
    "                preprocess=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f01bf3",
   "metadata": {},
   "source": [
    "We now take the initial action using `agent_cf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "080a2f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = agent.act(z=zs, \n",
    "               xt=states[:, 0], \n",
    "               xtm1=None, \n",
    "               atm1=None, \n",
    "               preprocess=True)\n",
    "a0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbef31",
   "metadata": {},
   "source": [
    "We now take the second action using `agent_cf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97cb571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = agent.act(z=zs, \n",
    "               xt=states[:, 1], \n",
    "               xtm1=states[:, 0], \n",
    "               atm1=a0, \n",
    "               preprocess=True)\n",
    "a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
